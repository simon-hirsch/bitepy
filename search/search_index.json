{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#welcome-to-the-battery-intraday-trading-engine-documentation","title":"Welcome to the Battery Intraday Trading Engine Documentation!","text":"<p>This is a Python high-frequency intraday trading engine for simulating the rolling intrinsic strategy on the European market, solved as a dynamic program. See our paper (tbd) for details on the method and visualizations of the results.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Process your own raw EPEX Spot Market Data to a suitable format</li> <li>Define battery and simulation settings</li> <li>Adapt the parameters of the dynamic programming optimization</li> <li>React to every single LOB update on the exchange</li> <li>Run yearly high-frequency trading simulations in minutes/hours</li> <li>Return and visualize results and statistics</li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>BitePy requires Python 3.8+ to run. The package can be easily installed via</p> <pre><code>pip install bitepy\n</code></pre>"},{"location":"#package","title":"Package","text":"<p>We divide our package into three major Python classes for preparing, running and visualizing the battery trading simulations. More detailed examples on how to use the package are given in the Tutorial.</p>"},{"location":"#data-preprocessing","title":"Data Preprocessing","text":"<p>Our <code>Data</code> class allows users to read-in raw zipped LOB Data from EPEX (2020 and later), process them accordingly and save each trading day as a separate CSV file. All Data is ultimately stored in UTC timezone format. We show and test this for German Market Data of the years 2020 and 2021, specifically using the 1h products of the continuous intraday market, but this can easily be adapted to other regions or other products. Inputs to the parsing function simply are the <code>start-day</code> and <code>end-day</code> of the data we want to parse, plus the <code>path</code> to the zipped EPEX market data.</p>"},{"location":"#simulation","title":"Simulation","text":"<p>The <code>Simulation</code> class enables users to initialize simulation instances, set parameters, load the preprocessed LOB Data into the simulation, run the simulation, and return results. Conceptually, you first set the parameters of the simulation (battery, dynamic programming, and simulation settings), then decide which days of LOB data to feed, before subsequently running the simulation for the desired amount of time. Order book traversals and optimizations happen in C++, while pre-/post-processing and settings are done in Python. Results are returned as Pandas dataframes and can be fed into the post-processing described below.</p>"},{"location":"#results-postprocessing","title":"Results Postprocessing","text":"<p>Our <code>Results</code> class, gives users some tools to visualize the final schedule, as determined by the rolling intrinsic simulation, and evaluate some key statistics. Of course, the user is encouraged to look at all simulation outputs in detail to understand the intricacies of the battery's trading behavior.</p>"},{"location":"#tutorial","title":"Tutorial","text":"<p>We give concrete usage examples and explanations to all the classes discussed above in our Jupyter Notebook.</p> <p>To reduce data-loading times, we encourage users to follow the flow of first creating LOB data CSV files with our <code>Data</code> class, but then creating LOB data binaries with our <code>Simulation</code> class, before running any simulations. Alternatively, users can also directly pass LOB Pandas dataframes to the simulation, at the cost of additional data-loading times.</p>"},{"location":"about/","title":"About","text":""},{"location":"about/#about-bitepy","title":"About BitePy","text":"<p>This Battery Intrade Trading Engine is the simulation code base for the paper (tbd) by Schaurecker et al. (2025). The base code is written in C++ and wrapped in Python for ease of use. Further features might be added in the near future, and this documentation will be updated accordingly.</p> <p>This code is written by David Schaurecker @dschaurecker, with pythonic support by Simon Hirsch @simon-hirsch, and is based on the work of his co-authors and himself. The Python code is open-source and can be used by anyone, but please cite the paper if you use it in your own work. We are happy to receive feedback and suggestions for improvement!</p>"},{"location":"data/","title":"Data Preprocessing","text":"<p>Our <code>Data</code> class allows users to read-in raw zipped LOB Data from EPEX (2020 and later), process them accordingly and save each trading day as a separate CSV file. All Data is ultimately stored in UTC timezone format. We show and test this for German Market Data of the years 2020 and 2021, specifically using the 1h products of the continuous intraday market, but this can easily be adapted to other regions or other products. Inputs to the parsing function simply are the <code>start-day</code> and <code>end-day</code> of the data we want to parse, plus the <code>path</code> to the zipped EPEX market data.</p> Source code in <code>bitepy/data.py</code> <pre><code>class Data:\n    def __init__(self):\n        \"\"\"Initialize a Data instance.\"\"\"\n        pass\n\n    def _load_csv(self, file_path):\n        \"\"\"\n        Load a single zipped CSV file with specified dtypes.\n        \"\"\"\n        df = pd.read_csv(\n            file_path,\n            compression=\"zip\",\n            dtype={\n                \"id\": np.int64,\n                \"initial\": np.int64,\n                \"side\": \"string\",\n                \"start\": \"string\",\n                \"transaction\": \"string\",\n                \"validity\": \"string\",\n                \"price\": np.float64,\n                \"quantity\": np.float64,\n            },\n        )\n        df.rename(columns={\"Unnamed: 0\": \"id\"}, inplace=True)\n        ids = df[\"id\"].to_numpy(dtype=np.int64).tolist()\n        initials = df[\"initial\"].to_numpy(dtype=np.int64).tolist()\n        sides = df[\"side\"].to_numpy(dtype=\"str\").tolist()\n        starts = df[\"start\"].to_numpy(dtype=\"str\").tolist()\n        transactions = df[\"transaction\"].to_numpy(dtype=\"str\").tolist()\n        validities = df[\"validity\"].to_numpy(dtype=\"str\").tolist()\n        prices = df[\"price\"].to_numpy(dtype=np.float64).tolist()\n        quantities = df[\"quantity\"].to_numpy(dtype=np.float64).tolist()\n        return ids, initials, sides, starts, transactions, validities, prices, quantities\n\n    def _read_id_table_2020(self, timestamp, datapath):\n        year = timestamp.strftime(\"%Y\")\n        month = timestamp.strftime(\"%m\")\n        datestr = \"Continuous_Orders_DE_\" + timestamp.strftime(\"%Y%m%d\")\n\n        # Get file name of zip-file and CSV file within the zip file\n        file_list = os.listdir(f\"{datapath}/{year}/{month}\")\n        zip_file_name = [i for i in file_list if datestr in i][0]\n        csv_file_name = zip_file_name[:-4]\n\n        # Read data from the CSV inside the zip file\n        zip_file = ZipFile(f\"{datapath}/{year}/{month}/\" + zip_file_name)\n        df = (pd.read_csv(zip_file.open(csv_file_name), sep=\";\", decimal=\".\")\n              .drop_duplicates(subset=[\"Order ID\", \"Initial ID\", \"Action code\", \"Validity time\", \"Price\", \"Quantity\"])\n              .loc[lambda x: x[\"Is User Defined Block\"] == 0]\n              .loc[lambda x: (x[\"Product\"] == \"Intraday_Hour_Power\") | (x[\"Product\"] == \"XBID_Hour_Power\")]\n              .loc[lambda x: (x[\"Action code\"] == \"A\") | (x[\"Action code\"] == \"D\") | (x[\"Action code\"] == \"C\") | (x[\"Action code\"] == \"I\")]\n              .drop([\"Delivery area\", \"Execution restriction\", \"Market area\", \"Parent ID\", \"Delivery End\",\n                     \"Currency\", \"Product\", \"isOTC\", \"Is User Defined Block\", \"Unnamed: 20\", \"RevisionNo\", \"Entry time\"],\n                    axis=1)\n              .rename({\"Order ID\": \"order\",\n                       \"Initial ID\": \"initial\",\n                       \"Delivery Start\": \"start\",\n                       \"Side\": \"side\",\n                       \"Price\": \"price\",\n                       \"Volume\": \"volume\",\n                       \"Validity time\": \"validity\",\n                       \"Action code\": \"action\",\n                       \"Transaction Time\": \"transaction\",\n                       \"Quantity\": \"quantity\"}, axis=1)\n              .assign(start=lambda x: pd.to_datetime(x.start, format=\"%Y-%m-%dT%H:%M:%SZ\"))\n              .assign(validity=lambda x: pd.to_datetime(x.validity, format=\"%Y-%m-%dT%H:%M:%SZ\"))\n              .assign(transaction=lambda x: pd.to_datetime(x.transaction, format=\"%Y-%m-%dT%H:%M:%S.%fZ\"))\n              )\n\n        # Remove iceberg orders\n        iceberg_IDs = df.loc[df[\"action\"] == \"I\", \"initial\"].unique()\n        df = df.loc[~df[\"initial\"].isin(iceberg_IDs)]\n\n        # Process change messages (action code 'C')\n        change_messages = df[df[\"action\"] == \"C\"].drop_duplicates(subset=[\"order\"], keep=\"first\")\n        not_added = change_messages[~(change_messages[\"order\"].isin(df.loc[df[\"action\"] == \"A\", \"order\"]))]\n        change_messages = change_messages[~(change_messages[\"order\"].isin(not_added[\"order\"]))]\n\n        change_exists = change_messages.shape[0] &gt; 0\n        change_counter = 0\n        while change_exists:\n            indexer_messA_with_change = df[(df[\"order\"].isin(change_messages[\"order\"])) &amp; (df[\"action\"] == \"A\")] \\\n                .sort_values(\"transaction\").groupby(\"order\").tail(1).index\n\n            df[\"df_index_copy\"] = df.index\n            merged = pd.merge(change_messages, df.loc[indexer_messA_with_change], on='order')\n            df.loc[merged[\"df_index_copy\"].to_numpy(), \"validity\"] = merged[\"transaction_x\"].to_numpy()\n\n            # Change the action code from \"C\" to \"A\" for processed messages\n            df.loc[df.index.isin(change_messages.index), \"action\"] = \"A\"\n            df.drop(\"df_index_copy\", axis=1, inplace=True)\n\n            # Redo the procedure for remaining change messages\n            change_messages = df[df[\"action\"] == \"C\"].drop_duplicates(subset=[\"order\"], keep=\"first\")\n            not_added = change_messages[~(change_messages[\"order\"].isin(df.loc[df[\"action\"] == \"A\", \"order\"]))]\n            change_messages = change_messages[~(change_messages[\"order\"].isin(not_added[\"order\"]))]\n            change_exists = change_messages.shape[0] &gt; 0\n            change_counter += 1\n\n        # Process cancel messages (action code 'D')\n        cancel_messages = df[df[\"action\"] == \"D\"]\n        not_added = cancel_messages[~(cancel_messages[\"order\"].isin(df.loc[df[\"action\"] == \"A\", \"order\"]))]\n        cancel_messages = cancel_messages[~(cancel_messages[\"order\"].isin(not_added[\"order\"]))]\n\n        indexer_messA_with_cancel = df[(df[\"order\"].isin(cancel_messages[\"order\"])) &amp; (df[\"action\"] == \"A\")] \\\n            .sort_values(\"transaction\").groupby(\"order\").tail(1).index\n        df[\"df_index_copy\"] = df.index\n        merged = pd.merge(cancel_messages, df.loc[indexer_messA_with_cancel], on='order')\n        df.loc[merged[\"df_index_copy\"].to_numpy(), \"validity\"] = merged[\"transaction_x\"].to_numpy()\n\n        df = df.loc[lambda x: ~(x[\"action\"] == \"D\")]\n        df = df.drop([\"order\", \"action\", \"df_index_copy\"], axis=1)\n\n        # Reorder and format columns\n        newOrder = [\"initial\", \"side\", \"start\", \"transaction\", \"validity\", \"price\", \"quantity\"]\n        df = df[newOrder]\n        df['side'] = df['side'].str.upper()\n\n        df[\"start\"] = df[\"start\"].dt.tz_localize('UTC').dt.strftime('%Y-%m-%dT%H:%M:%SZ')\n        df[\"transaction\"] = df[\"transaction\"].dt.tz_localize('UTC').dt.strftime('%Y-%m-%dT%H:%M:%S.%f').str[:-3] + 'Z'\n        df[\"validity\"] = df[\"validity\"].dt.tz_localize('UTC').dt.strftime('%Y-%m-%dT%H:%M:%S.%f').str[:-3] + 'Z'\n\n        return df\n\n    def _read_id_table_2021(self, timestamp, datapath):\n        year = timestamp.strftime(\"%Y\")\n        month = timestamp.strftime(\"%m\")\n        datestr = \"Continuous_Orders-DE-\" + timestamp.strftime(\"%Y%m%d\")\n\n        # Get file name of zip-file and CSV file within the zip file\n        file_list = os.listdir(f\"{datapath}/{year}/{month}\")\n        zip_file_name = [i for i in file_list if datestr in i][0]\n        csv_file_name = zip_file_name[:-4]\n\n        # Read data from the CSV inside the zip file\n        zip_file = ZipFile(f\"{datapath}/{year}/{month}/\" + zip_file_name)\n        df = (pd.read_csv(zip_file.open(csv_file_name), sep=\",\", decimal=\".\", skiprows=1)\n              .drop_duplicates(subset=[\"OrderId\", \"InitialId\", \"ActionCode\", \"ValidityTime\", \"Price\", \"Quantity\"])\n              .loc[lambda x: x[\"UserDefinedBlock\"] == \"N\"]\n              .loc[lambda x: (x[\"Product\"] == \"Intraday_Hour_Power\") | (x[\"Product\"] == \"XBID_Hour_Power\")]\n              .loc[lambda x: (x[\"ActionCode\"] == \"A\") | (x[\"ActionCode\"] == \"D\") | (x[\"ActionCode\"] == \"C\") | (x[\"ActionCode\"] == \"I\")]\n              .drop([\"LinkedBasketId\", \"DeliveryArea\", \"ParentId\", \"DeliveryEnd\", \"Currency\", \"Product\",\n                     \"UserDefinedBlock\", \"RevisionNo\", \"ExecutionRestriction\", \"CreationTime\", \"QuantityUnit\",\n                     \"Volume\", \"VolumeUnit\"], axis=1)\n              .rename({\"OrderId\": \"order\",\n                       \"InitialId\": \"initial\",\n                       \"DeliveryStart\": \"start\",\n                       \"Side\": \"side\",\n                       \"Price\": \"price\",\n                       \"Volume\": \"volume\",\n                       \"ValidityTime\": \"validity\",\n                       \"ActionCode\": \"action\",\n                       \"TransactionTime\": \"transaction\",\n                       \"Quantity\": \"quantity\"}, axis=1)\n              .assign(start=lambda x: pd.to_datetime(x.start, format=\"%Y-%m-%dT%H:%M:%SZ\"))\n              .assign(validity=lambda x: pd.to_datetime(x.validity, format=\"%Y-%m-%dT%H:%M:%SZ\"))\n              .assign(transaction=lambda x: pd.to_datetime(x.transaction, format=\"%Y-%m-%dT%H:%M:%S.%fZ\"))\n              )\n        # Remove iceberg orders\n        iceberg_IDs = df.loc[df[\"action\"] == \"I\", \"initial\"].unique()\n        df = df.loc[~df[\"initial\"].isin(iceberg_IDs)]\n\n        # Process change messages (action code 'C')\n        change_messages = df[df[\"action\"] == \"C\"].drop_duplicates(subset=[\"order\"], keep=\"first\")\n        not_added = change_messages[~(change_messages[\"order\"].isin(df.loc[df[\"action\"] == \"A\", \"order\"]))]\n        change_messages = change_messages[~(change_messages[\"order\"].isin(not_added[\"order\"]))]\n\n        change_exists = change_messages.shape[0] &gt; 0\n        change_counter = 0\n        while change_exists:\n            indexer_messA_with_change = df[(df[\"order\"].isin(change_messages[\"order\"])) &amp; (df[\"action\"] == \"A\")] \\\n                .sort_values(\"transaction\").groupby(\"order\").tail(1).index\n\n            df[\"df_index_copy\"] = df.index\n            merged = pd.merge(change_messages, df.loc[indexer_messA_with_change], on='order')\n            df.loc[merged[\"df_index_copy\"].to_numpy(), \"validity\"] = merged[\"transaction_x\"].to_numpy()\n\n            # Change the action code from \"C\" to \"A\" so it can be processed in the next iteration\n            df.loc[df.index.isin(change_messages.index), \"action\"] = \"A\"\n            df.drop(\"df_index_copy\", axis=1, inplace=True)\n\n            # Redo procedure for remaining change messages\n            change_messages = df[df[\"action\"] == \"C\"].drop_duplicates(subset=[\"order\"], keep=\"first\")\n            not_added = change_messages[~(change_messages[\"order\"].isin(df.loc[df[\"action\"] == \"A\", \"order\"]))]\n            change_messages = change_messages[~(change_messages[\"order\"].isin(not_added[\"order\"]))]\n            change_exists = change_messages.shape[0] &gt; 0\n            change_counter += 1\n\n        # Process cancel messages (action code 'D')\n        cancel_messages = df[df[\"action\"] == \"D\"]\n        not_added = cancel_messages[~(cancel_messages[\"order\"].isin(df.loc[df[\"action\"] == \"A\", \"order\"]))]\n        cancel_messages = cancel_messages[~(cancel_messages[\"order\"].isin(not_added[\"order\"]))]\n\n        indexer_messA_with_cancel = df[(df[\"order\"].isin(cancel_messages[\"order\"])) &amp; (df[\"action\"] == \"A\")] \\\n            .sort_values(\"transaction\").groupby(\"order\").tail(1).index\n        df[\"df_index_copy\"] = df.index\n        merged = pd.merge(cancel_messages, df.loc[indexer_messA_with_cancel], on='order')\n        df.loc[merged[\"df_index_copy\"].to_numpy(), \"validity\"] = merged[\"transaction_x\"].to_numpy()\n\n        df = df.loc[lambda x: ~(x[\"action\"] == \"D\")]\n        df = df.drop([\"order\", \"action\", \"df_index_copy\"], axis=1)\n\n        df[\"start\"] = df[\"start\"].dt.tz_localize('UTC').dt.strftime('%Y-%m-%dT%H:%M:%SZ')\n        df[\"transaction\"] = df[\"transaction\"].dt.tz_localize('UTC').dt.strftime('%Y-%m-%dT%H:%M:%S.%f').str[:-3] + 'Z'\n        df[\"validity\"] = df[\"validity\"].dt.tz_localize('UTC').dt.strftime('%Y-%m-%dT%H:%M:%S.%f').str[:-3] + 'Z'\n\n        return df\n\n    def parse_market_data(self, start_date_str: str, end_date_str: str, marketdatapath: str, savepath: str, verbose: bool = True):\n        \"\"\"\n        Parse EPEX market data between two dates and save processed zipped CSV files.\n\n        This method sequentially loads and processes the raw market data files (zipped order book data)\n        provided by EPEX. It converts the raw data into a sorted CSV file for each day in UTC time format.\n\n        The processing constructs the file name based on the timestamp,\n        reads the zipped CSV file (using a different CSV format and separator compared to 2020),\n        processes the data by removing duplicates, filtering rows, renaming columns,\n        and converting timestamp columns to UTC ISO 8601 format.\n        Additional processing is done to handle change and cancel messages.\n\n        Args:\n            start_date_str (str): Start date string in the format \"YYYY-MM-DD\" (no time zone).\n            end_date_str (str): End date string in the format \"YYYY-MM-DD\" (no time zone).\n            marketdatapath (str): Path to the market data folder containing yearly/monthly subfolders with zipped files.\n            savepath (str): Directory path where the parsed CSV files should be saved.\n            verbose (bool, optional): If True, print progress messages. Defaults to True.\n        \"\"\"\n        if not os.path.exists(savepath):\n            os.makedirs(savepath)\n\n        start_date = pd.Timestamp(start_date_str)\n        end_date = pd.Timestamp(end_date_str)\n\n        if start_date &gt; end_date:\n            raise ValueError(\"Error: Start date is after end date.\")\n        if start_date.year &lt; 2020:\n            raise ValueError(\"Error: Years before 2020 are not supported.\")\n\n        dates = pd.date_range(start_date, end_date, freq=\"D\")\n        df1 = pd.DataFrame()\n        df2 = pd.DataFrame()\n\n        with tqdm(total=len(dates), desc=\"Loading and saving CSV data\", ncols=100, disable=not verbose) as pbar:\n            for dt1 in dates:\n                pbar.set_description(f\"Currently loading and saving date {str(dt1.date())} ... \")\n                df1 = df2\n                df2 = pd.DataFrame()\n                dt2 = dt1 + pd.Timedelta(days=1)\n                if df1.empty:\n                    if dt1.year == 2020:\n                        df1 = self._read_id_table_2020(dt1, marketdatapath)\n                    elif dt1.year &gt;= 2021:\n                        df1 = self._read_id_table_2021(dt1, marketdatapath)\n                    else:\n                        raise ValueError(\"Error: Year not &gt;= 2020\")\n                if dt2 &lt;= end_date:\n                    if dt2.year == 2020:\n                        df2 = self._read_id_table_2020(dt2, marketdatapath)\n                    elif dt2.year &gt;= 2021:\n                        df2 = self._read_id_table_2021(dt2, marketdatapath)\n                    else:\n                        raise ValueError(\"Error: Year not &gt;= 2020\")\n\n                df = pd.concat([df1, df2])\n                df = df.sort_values(by='transaction')\n                df['transaction_date'] = pd.to_datetime(df['transaction']).dt.date  # Extract date part\n                grouped = df.groupby('transaction_date')\n\n                save_date = dt1.date()\n                group = grouped.get_group(save_date)\n                daily_filename = f\"{savepath}orderbook_{save_date}.csv\"\n                compression_options = dict(method='zip', archive_name=f'{daily_filename.split(\"/\")[-1]}')\n                group.drop(columns='transaction_date').sort_values(by='transaction').fillna(\"\").to_csv(f'{daily_filename}.zip', compression=compression_options)\n                pbar.update(1)\n\n        print(\"\\nWriting CSV data completed.\")\n\n    def create_bins_from_csv(self, csv_list: list, save_path: str, verbose: bool = True):\n        \"\"\"\n        Convert zipped CSV files of pre-processed order book data into binary files.\n\n        This method sequentially loads each previously generated zipped CSV file, converts it to a binary format using the C++ simulation\n        extension, and saves the binary file in the specified directory. Binary files allow for much (10x) quicker loading\n        of the data at runtime.\n\n        Args:\n            csv_list (list): List of file paths to the zipped CSV files containing pre-processed order book data.\n            save_path (str): Directory path where the binary files should be saved. The binary files will use the same base name as the CSV files.\n            verbose (bool, optional): If True, print progress messages. Defaults to True.\n        \"\"\"\n        if not os.path.exists(save_path):\n            os.makedirs(save_path)\n\n        _sim = Simulation_cpp()\n        with tqdm(total=len(csv_list), desc=\"Writing Binaries\", ncols=100, disable=not verbose) as pbar:\n            for csv_file_path in csv_list:\n                filename = os.path.basename(csv_file_path)\n                bin_file_path = os.path.join(save_path, filename.replace(\".csv.zip\", \".bin\"))\n                pbar.set_description(f\"Currently saving binary {bin_file_path.split('/')[-1]} ... \")\n                ids, initials, sides, starts, transactions, validities, prices, quantities = self._load_csv(csv_file_path)\n                _sim.writeOrderBinFromPandas(\n                    bin_file_path,\n                    ids,\n                    initials,\n                    sides,\n                    starts,\n                    transactions,\n                    validities,\n                    prices,\n                    quantities,\n                )\n                pbar.update(1)\n\n        print(\"\\nWriting Binaries completed.\")\n</code></pre>"},{"location":"data/#bitepy.Data.__init__","title":"<code>__init__()</code>","text":"<p>Initialize a Data instance.</p> Source code in <code>bitepy/data.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize a Data instance.\"\"\"\n    pass\n</code></pre>"},{"location":"data/#bitepy.Data.create_bins_from_csv","title":"<code>create_bins_from_csv(csv_list, save_path, verbose=True)</code>","text":"<p>Convert zipped CSV files of pre-processed order book data into binary files.</p> <p>This method sequentially loads each previously generated zipped CSV file, converts it to a binary format using the C++ simulation extension, and saves the binary file in the specified directory. Binary files allow for much (10x) quicker loading of the data at runtime.</p> <p>Parameters:</p> Name Type Description Default <code>csv_list</code> <code>list</code> <p>List of file paths to the zipped CSV files containing pre-processed order book data.</p> required <code>save_path</code> <code>str</code> <p>Directory path where the binary files should be saved. The binary files will use the same base name as the CSV files.</p> required <code>verbose</code> <code>bool</code> <p>If True, print progress messages. Defaults to True.</p> <code>True</code> Source code in <code>bitepy/data.py</code> <pre><code>def create_bins_from_csv(self, csv_list: list, save_path: str, verbose: bool = True):\n    \"\"\"\n    Convert zipped CSV files of pre-processed order book data into binary files.\n\n    This method sequentially loads each previously generated zipped CSV file, converts it to a binary format using the C++ simulation\n    extension, and saves the binary file in the specified directory. Binary files allow for much (10x) quicker loading\n    of the data at runtime.\n\n    Args:\n        csv_list (list): List of file paths to the zipped CSV files containing pre-processed order book data.\n        save_path (str): Directory path where the binary files should be saved. The binary files will use the same base name as the CSV files.\n        verbose (bool, optional): If True, print progress messages. Defaults to True.\n    \"\"\"\n    if not os.path.exists(save_path):\n        os.makedirs(save_path)\n\n    _sim = Simulation_cpp()\n    with tqdm(total=len(csv_list), desc=\"Writing Binaries\", ncols=100, disable=not verbose) as pbar:\n        for csv_file_path in csv_list:\n            filename = os.path.basename(csv_file_path)\n            bin_file_path = os.path.join(save_path, filename.replace(\".csv.zip\", \".bin\"))\n            pbar.set_description(f\"Currently saving binary {bin_file_path.split('/')[-1]} ... \")\n            ids, initials, sides, starts, transactions, validities, prices, quantities = self._load_csv(csv_file_path)\n            _sim.writeOrderBinFromPandas(\n                bin_file_path,\n                ids,\n                initials,\n                sides,\n                starts,\n                transactions,\n                validities,\n                prices,\n                quantities,\n            )\n            pbar.update(1)\n\n    print(\"\\nWriting Binaries completed.\")\n</code></pre>"},{"location":"data/#bitepy.Data.parse_market_data","title":"<code>parse_market_data(start_date_str, end_date_str, marketdatapath, savepath, verbose=True)</code>","text":"<p>Parse EPEX market data between two dates and save processed zipped CSV files.</p> <p>This method sequentially loads and processes the raw market data files (zipped order book data) provided by EPEX. It converts the raw data into a sorted CSV file for each day in UTC time format.</p> <p>The processing constructs the file name based on the timestamp, reads the zipped CSV file (using a different CSV format and separator compared to 2020), processes the data by removing duplicates, filtering rows, renaming columns, and converting timestamp columns to UTC ISO 8601 format. Additional processing is done to handle change and cancel messages.</p> <p>Parameters:</p> Name Type Description Default <code>start_date_str</code> <code>str</code> <p>Start date string in the format \"YYYY-MM-DD\" (no time zone).</p> required <code>end_date_str</code> <code>str</code> <p>End date string in the format \"YYYY-MM-DD\" (no time zone).</p> required <code>marketdatapath</code> <code>str</code> <p>Path to the market data folder containing yearly/monthly subfolders with zipped files.</p> required <code>savepath</code> <code>str</code> <p>Directory path where the parsed CSV files should be saved.</p> required <code>verbose</code> <code>bool</code> <p>If True, print progress messages. Defaults to True.</p> <code>True</code> Source code in <code>bitepy/data.py</code> <pre><code>def parse_market_data(self, start_date_str: str, end_date_str: str, marketdatapath: str, savepath: str, verbose: bool = True):\n    \"\"\"\n    Parse EPEX market data between two dates and save processed zipped CSV files.\n\n    This method sequentially loads and processes the raw market data files (zipped order book data)\n    provided by EPEX. It converts the raw data into a sorted CSV file for each day in UTC time format.\n\n    The processing constructs the file name based on the timestamp,\n    reads the zipped CSV file (using a different CSV format and separator compared to 2020),\n    processes the data by removing duplicates, filtering rows, renaming columns,\n    and converting timestamp columns to UTC ISO 8601 format.\n    Additional processing is done to handle change and cancel messages.\n\n    Args:\n        start_date_str (str): Start date string in the format \"YYYY-MM-DD\" (no time zone).\n        end_date_str (str): End date string in the format \"YYYY-MM-DD\" (no time zone).\n        marketdatapath (str): Path to the market data folder containing yearly/monthly subfolders with zipped files.\n        savepath (str): Directory path where the parsed CSV files should be saved.\n        verbose (bool, optional): If True, print progress messages. Defaults to True.\n    \"\"\"\n    if not os.path.exists(savepath):\n        os.makedirs(savepath)\n\n    start_date = pd.Timestamp(start_date_str)\n    end_date = pd.Timestamp(end_date_str)\n\n    if start_date &gt; end_date:\n        raise ValueError(\"Error: Start date is after end date.\")\n    if start_date.year &lt; 2020:\n        raise ValueError(\"Error: Years before 2020 are not supported.\")\n\n    dates = pd.date_range(start_date, end_date, freq=\"D\")\n    df1 = pd.DataFrame()\n    df2 = pd.DataFrame()\n\n    with tqdm(total=len(dates), desc=\"Loading and saving CSV data\", ncols=100, disable=not verbose) as pbar:\n        for dt1 in dates:\n            pbar.set_description(f\"Currently loading and saving date {str(dt1.date())} ... \")\n            df1 = df2\n            df2 = pd.DataFrame()\n            dt2 = dt1 + pd.Timedelta(days=1)\n            if df1.empty:\n                if dt1.year == 2020:\n                    df1 = self._read_id_table_2020(dt1, marketdatapath)\n                elif dt1.year &gt;= 2021:\n                    df1 = self._read_id_table_2021(dt1, marketdatapath)\n                else:\n                    raise ValueError(\"Error: Year not &gt;= 2020\")\n            if dt2 &lt;= end_date:\n                if dt2.year == 2020:\n                    df2 = self._read_id_table_2020(dt2, marketdatapath)\n                elif dt2.year &gt;= 2021:\n                    df2 = self._read_id_table_2021(dt2, marketdatapath)\n                else:\n                    raise ValueError(\"Error: Year not &gt;= 2020\")\n\n            df = pd.concat([df1, df2])\n            df = df.sort_values(by='transaction')\n            df['transaction_date'] = pd.to_datetime(df['transaction']).dt.date  # Extract date part\n            grouped = df.groupby('transaction_date')\n\n            save_date = dt1.date()\n            group = grouped.get_group(save_date)\n            daily_filename = f\"{savepath}orderbook_{save_date}.csv\"\n            compression_options = dict(method='zip', archive_name=f'{daily_filename.split(\"/\")[-1]}')\n            group.drop(columns='transaction_date').sort_values(by='transaction').fillna(\"\").to_csv(f'{daily_filename}.zip', compression=compression_options)\n            pbar.update(1)\n\n    print(\"\\nWriting CSV data completed.\")\n</code></pre>"},{"location":"results/","title":"Results Postprocessing","text":"<p>Our <code>Results</code> class, gives users some tools to visualize the final schedule, as determined by the rolling intrinsic simulation, and evaluate some key statistics. Of course, the user is encouraged to look at all simulation outputs in detail to understand the intricacies of the battery's trading behavior.</p> Source code in <code>bitepy/results.py</code> <pre><code>class Results:\n    def __init__(self, logs: dict):\n        \"\"\"\n        Initialize a Simulation instance.\n\n        Args:\n            logs (dict): A dictionary containing the get_logs() output of the simulation class.\n        \"\"\"\n\n        self.logs = logs\n\n    def get_total_reward(self):\n        return np.round(self.logs[\"decision_record\"]['real_reward'].sum(),2)\n\n    def plot_decision_chart(self,lleft: int = 0,lright: int = -1):\n        \"\"\"\n        Plot the storage, market-position, and reward of the agent over the selected simulation period.\n\n        Args:\n            lleft (int): The left index of the simulation period.\n            lright (int): The right index of the simulation period.\n        \"\"\"\n        df = self.logs[\"decision_record\"]\n\n        # plot storage, position, and reward where reward is in a seperate axis below\n        fig1, ax1 = plt.subplots(figsize=(18, 10))\n        ax2 = ax1.twinx()\n        ax1.plot(df[\"hour\"][lleft:lright], df['storage'][lleft:lright], color='blue')\n        #plot position as points not line\n        ax1.plot(df[\"hour\"][lleft:lright], df['position'][lleft:lright], 'o', color='red')\n\n\n        ax2.plot(df[\"hour\"][lleft:lright], df['real_reward'][lleft:lright], color='green')\n        ax1.set_xlabel('Time ')\n        ax1.set_ylabel('Storage (MWh)')\n        ax2.set_ylabel('Reward (\u20ac)')\n        # plot a horizontal line at 0\n        ax2.axhline(y=0, color='green', linestyle='--', linewidth=1.5, alpha=0.5)\n        ax1.legend(['storage', 'position'], loc='upper left')\n        ax2.legend(['reward'], loc='upper right')\n        #set gridlines\n        # plot vertical grid line for each hour\n\n        # # array for each hour between lleft and lright\n        # hours = pd.date_range(start=df[\"hour\"].iloc[lleft], end=df[\"hour\"].iloc[lright], freq='D')\n        # for hour in hours:\n        #     ax1.axvline(x=hour, color='gray', linestyle='--', linewidth=0.5)\n\n        ax1.grid(True, alpha=0.5)\n        plt.show()\n\n        fig1, ax1 = plt.subplots(figsize=(18, 10))\n        #plot cumulative reward\n        ax1.plot(df[\"hour\"][lleft:lright], df['real_reward'].cumsum()[lleft:lright], color='blue')\n        ax1.set_xlabel('Time ')\n        ax1.set_ylabel('Cumulative Reward (\u20ac)')\n        ax1.grid(True, alpha=0.5)\n        plt.show()\n\n    def plot_heatmap(self):\n        \"\"\"\n        Plot a heatmap of the final storage positions and visualize the executed orders over the simulation period.\n        Heatmap plots adapted from: https://github.com/bitstoenergy/iclr-smartmeteranalytics by Markus Kreft.\n        \"\"\"\n\n        df = self.logs[\"decision_record\"]\n        df.index = df[\"hour\"]\n        df = df.drop(columns=[\"hour\"])\n\n        exec_df = self.logs[\"executed_orders\"].drop(columns=[\"dp_run\", \"last_solve_time\", \"final_pos\", \"final_stor\"])\n        exec_df.set_index('hour', inplace=True)\n\n        # Create an empty list to store results\n        daily_volumes = []\n\n        # Iterate through unique dates in df\n        for day in np.unique(df.index.date):\n            # Filter the data for the current day\n            daily_data = exec_df.loc[exec_df.index.date == day]\n\n            # Calculate the largest daily volume and the summed daily volume\n            largest_daily_vol = daily_data[\"volume\"].max()\n            summed_daily_vol = daily_data[\"volume\"].sum()\n\n            # Append the results as a dictionary\n            daily_volumes.append({\n                \"date\": day,\n                \"max_vol\": largest_daily_vol,\n                \"summed_vol\": summed_daily_vol\n            })\n\n        # Create a DataFrame from the list of dictionaries\n        daily_volumes_df = pd.DataFrame(daily_volumes)\n\n        # Set the 'date' column as the index\n        daily_volumes_df.set_index(\"date\", inplace=True)\n        daily_volumes_df.index = pd.to_datetime(daily_volumes_df.index).tz_localize(\"UTC\")\n\n\n        fig = hm.HeatmapFigure(df, daily_volumes_df, 'storage', interval_minutes=60, figsize=(14, 8))\n        plt.show()\n</code></pre>"},{"location":"results/#bitepy.Results.__init__","title":"<code>__init__(logs)</code>","text":"<p>Initialize a Simulation instance.</p> <p>Parameters:</p> Name Type Description Default <code>logs</code> <code>dict</code> <p>A dictionary containing the get_logs() output of the simulation class.</p> required Source code in <code>bitepy/results.py</code> <pre><code>def __init__(self, logs: dict):\n    \"\"\"\n    Initialize a Simulation instance.\n\n    Args:\n        logs (dict): A dictionary containing the get_logs() output of the simulation class.\n    \"\"\"\n\n    self.logs = logs\n</code></pre>"},{"location":"results/#bitepy.Results.plot_decision_chart","title":"<code>plot_decision_chart(lleft=0, lright=-1)</code>","text":"<p>Plot the storage, market-position, and reward of the agent over the selected simulation period.</p> <p>Parameters:</p> Name Type Description Default <code>lleft</code> <code>int</code> <p>The left index of the simulation period.</p> <code>0</code> <code>lright</code> <code>int</code> <p>The right index of the simulation period.</p> <code>-1</code> Source code in <code>bitepy/results.py</code> <pre><code>def plot_decision_chart(self,lleft: int = 0,lright: int = -1):\n    \"\"\"\n    Plot the storage, market-position, and reward of the agent over the selected simulation period.\n\n    Args:\n        lleft (int): The left index of the simulation period.\n        lright (int): The right index of the simulation period.\n    \"\"\"\n    df = self.logs[\"decision_record\"]\n\n    # plot storage, position, and reward where reward is in a seperate axis below\n    fig1, ax1 = plt.subplots(figsize=(18, 10))\n    ax2 = ax1.twinx()\n    ax1.plot(df[\"hour\"][lleft:lright], df['storage'][lleft:lright], color='blue')\n    #plot position as points not line\n    ax1.plot(df[\"hour\"][lleft:lright], df['position'][lleft:lright], 'o', color='red')\n\n\n    ax2.plot(df[\"hour\"][lleft:lright], df['real_reward'][lleft:lright], color='green')\n    ax1.set_xlabel('Time ')\n    ax1.set_ylabel('Storage (MWh)')\n    ax2.set_ylabel('Reward (\u20ac)')\n    # plot a horizontal line at 0\n    ax2.axhline(y=0, color='green', linestyle='--', linewidth=1.5, alpha=0.5)\n    ax1.legend(['storage', 'position'], loc='upper left')\n    ax2.legend(['reward'], loc='upper right')\n    #set gridlines\n    # plot vertical grid line for each hour\n\n    # # array for each hour between lleft and lright\n    # hours = pd.date_range(start=df[\"hour\"].iloc[lleft], end=df[\"hour\"].iloc[lright], freq='D')\n    # for hour in hours:\n    #     ax1.axvline(x=hour, color='gray', linestyle='--', linewidth=0.5)\n\n    ax1.grid(True, alpha=0.5)\n    plt.show()\n\n    fig1, ax1 = plt.subplots(figsize=(18, 10))\n    #plot cumulative reward\n    ax1.plot(df[\"hour\"][lleft:lright], df['real_reward'].cumsum()[lleft:lright], color='blue')\n    ax1.set_xlabel('Time ')\n    ax1.set_ylabel('Cumulative Reward (\u20ac)')\n    ax1.grid(True, alpha=0.5)\n    plt.show()\n</code></pre>"},{"location":"results/#bitepy.Results.plot_heatmap","title":"<code>plot_heatmap()</code>","text":"<p>Plot a heatmap of the final storage positions and visualize the executed orders over the simulation period. Heatmap plots adapted from: https://github.com/bitstoenergy/iclr-smartmeteranalytics by Markus Kreft.</p> Source code in <code>bitepy/results.py</code> <pre><code>def plot_heatmap(self):\n    \"\"\"\n    Plot a heatmap of the final storage positions and visualize the executed orders over the simulation period.\n    Heatmap plots adapted from: https://github.com/bitstoenergy/iclr-smartmeteranalytics by Markus Kreft.\n    \"\"\"\n\n    df = self.logs[\"decision_record\"]\n    df.index = df[\"hour\"]\n    df = df.drop(columns=[\"hour\"])\n\n    exec_df = self.logs[\"executed_orders\"].drop(columns=[\"dp_run\", \"last_solve_time\", \"final_pos\", \"final_stor\"])\n    exec_df.set_index('hour', inplace=True)\n\n    # Create an empty list to store results\n    daily_volumes = []\n\n    # Iterate through unique dates in df\n    for day in np.unique(df.index.date):\n        # Filter the data for the current day\n        daily_data = exec_df.loc[exec_df.index.date == day]\n\n        # Calculate the largest daily volume and the summed daily volume\n        largest_daily_vol = daily_data[\"volume\"].max()\n        summed_daily_vol = daily_data[\"volume\"].sum()\n\n        # Append the results as a dictionary\n        daily_volumes.append({\n            \"date\": day,\n            \"max_vol\": largest_daily_vol,\n            \"summed_vol\": summed_daily_vol\n        })\n\n    # Create a DataFrame from the list of dictionaries\n    daily_volumes_df = pd.DataFrame(daily_volumes)\n\n    # Set the 'date' column as the index\n    daily_volumes_df.set_index(\"date\", inplace=True)\n    daily_volumes_df.index = pd.to_datetime(daily_volumes_df.index).tz_localize(\"UTC\")\n\n\n    fig = hm.HeatmapFigure(df, daily_volumes_df, 'storage', interval_minutes=60, figsize=(14, 8))\n    plt.show()\n</code></pre>"},{"location":"simulation/","title":"Simulation","text":"<p>The <code>Simulation</code> class enables users to initialize simulation instances, set parameters, load the preprocessed LOB Data into the simulation, run the simulation, and return results. Conceptually, you first set the parameters of the simulation (battery, dynamic programming, and simulation settings), then decide which days of LOB data to feed, before subsequently running the simulation for the desired amount of time. Order book traversals and optimizations happen in C++, while pre-/post-processing and settings are done in Python. Results are returned as Pandas dataframes and can be fed into the post-processing described below.</p> Source code in <code>bitepy/simulation.py</code> <pre><code>class Simulation:\n    def __init__(self, start_date: pd.Timestamp, end_date: pd.Timestamp,\n                 storage_max=10.,\n                 lin_deg_cost=4.,\n                 loss_in=0.95,\n                 loss_out=0.95,\n                 trading_fee=0.09,\n                 num_stor_states=11,\n                 tec_delay=0,\n                 fixed_solve_time=0,\n                 solve_frequency=0.,\n                 withdraw_max=10.,\n                 inject_max=10.):\n                #  forecast_horizon_start=10*60,\n                #  forecast_horizon_end=75):\n        \"\"\"\n        Initialize a Simulation instance.\n\n        Args:\n            start_date (pd.Timestamp): The start datetime of the simulation. Must be timezone aware.\n            end_date (pd.Timestamp): The end datetime of the simulation. Must be timezone aware.\n            storage_max (float, optional): The maximum storage capacity of the storage unit (MWh). Default is 10.0.\n            lin_deg_cost (float, optional): The linear degradation cost of the storage unit (\u20ac/MWh). Default is 4.0.\n            loss_in (float, optional): The injection efficiency of the storage unit (0-1]. Default is 0.95.\n            loss_out (float, optional): The withdrawal efficiency of the storage unit (0-1]. Default is 0.95.\n            trading_fee (float, optional): The trading fee for the exchange (\u20ac/MWh). Default is 0.09.\n            num_stor_states (int, optional): The number of storage states for dynamic programming. Default is 11.\n            tec_delay (int, optional): The technical delay of the storage unit (ms, &gt;= 0). Default is 0.\n            fixed_solve_time (int, optional): The fixed solve time for dynamic programming (ms, &gt;= 0 or -1 for realistic solve times). Default is 0.\n            solve_frequency (float, optional): The frequency at which the dynamic programming solver is run (min). Default is 0.0.\n            withdraw_max (float, optional): The maximum withdrawal power of the storage unit (MW). Default is 10.0.\n            inject_max (float, optional): The maximum injection power of the storage unit (MW). Default is 10.0.\n        \"\"\"\n        # forecast_horizon_start (int, optional): The start of the forecast horizon (min). Default is 600.\n        # forecast_horizon_end (int, optional): The end of the forecast horizon (min). Default is 75.\n\n        # write all the assertions\n        if start_date &gt;= end_date:\n            raise ValueError(\"start_date must be before end_date\")\n        if storage_max &lt; 0:\n            raise ValueError(\"storage_max must be &gt;= 0\")\n        if lin_deg_cost &lt; 0:\n            raise ValueError(\"lin_deg_cost must be &gt;= 0\")\n        if loss_in &lt; 0 or loss_in &gt; 1:\n            raise ValueError(\"loss_in must be in [0, 1]\")\n        if loss_out &lt; 0 or loss_out &gt; 1:\n            raise ValueError(\"loss_out must be in [0,1]\")\n        if trading_fee &lt; 0:\n            raise ValueError(\"trading_fee must be &gt;= 0\")\n        if num_stor_states &lt;= 0:\n            raise ValueError(\"num_stor_states must be &gt; 0\")\n        if tec_delay &lt; 0:\n            raise ValueError(\"tec_delay must be &gt;= 0\")\n        if fixed_solve_time &lt; 0:\n            if fixed_solve_time != -1:\n                raise ValueError(\"fixed_solve_time must be &gt;= 0 (or -1 for realistic solve times)\")\n        if solve_frequency &lt; 0:\n            raise ValueError(\"solve_frequency must be &gt;= 0\")\n        if withdraw_max &lt;= 0:\n            raise ValueError(\"withdraw_max must be &gt; 0\")\n        if inject_max &lt;= 0:\n            raise ValueError(\"inject_max must be &gt; 0\")\n        # if forecast_horizon_start &lt; 0:\n        #     raise ValueError(\"forecast_horizon_start must be &gt;= 0\")\n        # if forecast_horizon_end &lt; 0:\n        #     raise ValueError(\"forecast_horizon_end must be &gt;= 0\")\n        # if forecast_horizon_start &lt;= forecast_horizon_end:\n        #     raise ValueError(\"forecast_horizon_start must larger than forecast_horizon_end\")\n\n        self._sim_cpp = Simulation_cpp()\n\n        self._sim_cpp.params.storageMax = storage_max\n        self._sim_cpp.params.linDegCost = lin_deg_cost\n        self._sim_cpp.params.lossIn = loss_in\n        self._sim_cpp.params.lossOut = loss_out\n        self._sim_cpp.params.tradingFee = trading_fee\n        self._sim_cpp.params.numStorStates = num_stor_states\n        self._sim_cpp.params.pingDelay = tec_delay\n        self._sim_cpp.params.fixedSolveTime = fixed_solve_time\n        self._sim_cpp.params.dpFreq = solve_frequency\n        self._sim_cpp.params.withdrawMax = withdraw_max\n        self._sim_cpp.params.injectMax = inject_max\n        # self._sim_cpp.params.foreHorizonStart = forecast_horizon_start\n        # self._sim_cpp.params.foreHorizonEnd = forecast_horizon_end\n\n        # Set start and end date\n        if start_date &gt;= end_date:\n            raise ValueError(\"start_date must be before end_date\")\n        if start_date.tzinfo is None:\n            raise ValueError(\"start_date must be timezone aware\")\n        start_date = start_date.astimezone(pytz.utc)\n        self._sim_cpp.params.startMonth = start_date.month\n        self._sim_cpp.params.startDay = start_date.day\n        self._sim_cpp.params.startYear = start_date.year\n        self._sim_cpp.params.startHour = start_date.hour\n        if end_date.tzinfo is None:\n            raise ValueError(\"end_date must be timezone aware\")\n        end_date = end_date.astimezone(pytz.utc)\n        self._sim_cpp.params.endMonth = end_date.month\n        self._sim_cpp.params.endDay = end_date.day\n        self._sim_cpp.params.endYear = end_date.year\n        self._sim_cpp.params.endHour = end_date.hour\n\n    def add_bin_to_orderqueue(self, bin_data: str):\n        \"\"\"\n        Add an order binary file to the simulation's order queue.\n\n        Args:\n            bin_data (str): The path to the order binary file.\n        \"\"\"\n        self._sim_cpp.addOrderQueueFromBin(bin_data)\n\n    def add_df_to_orderqueue(self, df: pd.DataFrame):\n        \"\"\"\n        Add a DataFrame of orders to the simulation's order queue.\n\n        The DataFrame must have the same columns as the saved CSV files, with timestamps in UTC\n        (seconds and milliseconds).\n\n        Args:\n            df (pd.DataFrame): A DataFrame containing the orders to be added.\n\n        Processing Steps:\n            - Validate that the timestamp columns ('start', 'transaction', 'validity') are timezone aware.\n            - Ensure that all timestamps are in the same timezone.\n            - Convert all timestamps to UTC and format them in ISO 8601.\n        \"\"\"\n        if (df[\"start\"].dt.tz is None and df[\"transaction\"].dt.tz is None and df[\"validity\"].dt.tz is None):\n            raise ValueError(\"All timestamps of input df must be timezone aware\")\n        if not (df[\"start\"].dt.tz == df[\"transaction\"].dt.tz and df[\"start\"].dt.tz == df[\"validity\"].dt.tz):\n            raise ValueError(\"All timestamps of input df must be in the same timezone\")\n\n        df[\"start\"] = df[\"start\"].dt.tz_convert(\"UTC\")\n        df[\"transaction\"] = df[\"transaction\"].dt.tz_convert(\"UTC\")\n        df[\"validity\"] = df[\"validity\"].dt.tz_convert(\"UTC\")\n        df[\"start\"] = df[\"start\"].dt.tz_localize(None).dt.strftime('%Y-%m-%dT%H:%M:%SZ')\n        df[\"transaction\"] = df[\"transaction\"].dt.tz_localize(None).dt.strftime('%Y-%m-%dT%H:%M:%S.%f').str[:-3] + 'Z'\n        df[\"validity\"] = df[\"validity\"].dt.tz_localize(None).dt.strftime('%Y-%m-%dT%H:%M:%S.%f').str[:-3] + 'Z'\n\n        ids = df['id'].to_numpy(dtype=np.int64).tolist()\n        initials = df['initial'].to_numpy(dtype=np.int64).tolist()\n        sides = df['side'].to_numpy(dtype='str').tolist()\n        starts = df['start'].to_numpy(dtype='str').tolist()\n        transactions = df['transaction'].to_numpy(dtype='str').tolist()\n        validities = df['validity'].to_numpy(dtype='str').tolist()\n        prices = df['price'].to_numpy(dtype=np.float64).tolist()\n        quantities = df['quantity'].to_numpy(dtype=np.float64).tolist()\n\n        self._sim_cpp.addOrderQueueFromPandas(ids, initials, sides, starts, transactions, validities, prices, quantities)\n\n    # def add_forecast_from_df(self, df: pd.DataFrame):\n    #     \"\"\"\n    #     Add forecast data from a DataFrame to the simulation.\n\n    #     The DataFrame must contain the following columns:\n    #         - creation_time: The time when the forecast was created (timezone aware, up to millisecond precision).\n    #         - delivery_start: The start time of the delivery period (timezone aware).\n    #         - sell_price: The price at which the optimization will try to sell (\u20ac/MWh).\n    #         - buy_price: The price at which the optimization will try to buy (\u20ac/MWh).\n\n    #     Args:\n    #         df (pd.DataFrame): A DataFrame containing the forecast data.\n\n    #     Processing Steps:\n    #         - Validate that the 'creation_time' and 'delivery_start' columns are timezone aware and identical.\n    #         - Convert the timestamps to UTC and format them in ISO 8601.\n    #         - Pass the data to the simulation.\n    #     \"\"\"\n    #     if (df[\"creation_time\"].dt.tz is None and df[\"delivery_start\"].dt.tz is None):\n    #         raise ValueError(\"All timestamps of input df must be timezone aware\")\n    #     if not (df[\"creation_time\"].dt.tz == df[\"delivery_start\"].dt.tz):\n    #         raise ValueError(\"All timestamps of input df must be in the same timezone\")\n\n    #     df[\"creation_time\"] = df[\"creation_time\"].dt.tz_convert(\"UTC\")\n    #     df[\"delivery_start\"] = df[\"delivery_start\"].dt.tz_convert(\"UTC\")\n\n    #     df[\"creation_time\"] = df[\"creation_time\"].dt.tz_localize(None).dt.strftime('%Y-%m-%dT%H:%M:%S.%f').str[:-3] + 'Z'\n    #     df[\"delivery_start\"] = df[\"delivery_start\"].dt.tz_localize(None).dt.strftime('%Y-%m-%dT%H:%M:%SZ')\n\n    #     creation_time = df[\"creation_time\"].to_numpy(dtype='str').tolist()\n    #     delivery_start = df[\"delivery_start\"].to_numpy(dtype='str').tolist()\n    #     buy_price = df[\"buy_price\"].to_numpy(dtype=np.float64).tolist()\n    #     sell_price = df[\"sell_price\"].to_numpy(dtype=np.float64).tolist()\n\n    #     self._sim_cpp.loadForecastMapFromPandas(creation_time, delivery_start, buy_price, sell_price)\n\n    def get_data_bins_for_each_day(self, base_path: str, start_date: pd.Timestamp, end_date: pd.Timestamp):\n        \"\"\"\n        Generate a list of file paths for binary order book data for each day within a date range.\n\n        Args:\n            base_path (str): The base directory path where the binary files are stored.\n            start_date (pd.Timestamp): The start date of the range.\n            end_date (pd.Timestamp): The end date of the range.\n\n        Returns:\n            list: A list of file paths for each day's binary order book file.\n        \"\"\"\n        start_date_utc = start_date.tz_convert('UTC')\n        end_date_utc = end_date.tz_convert('UTC')\n\n        if base_path[-1] != '/':\n            base_path += '/'\n        base_path += \"orderbook_\"\n\n        paths = []\n        current_date = start_date_utc\n        while current_date &lt; end_date_utc + timedelta(days=1):\n            path = f\"{base_path}{current_date.strftime('%Y-%m-%d')}.bin\"\n            paths.append(path)\n            current_date += timedelta(days=1)\n        return paths\n\n    def run(self, data_path: str, verbose: bool = True):\n        \"\"\"\n        Execute the simulation using binary data files.\n\n        The files must be named as: orderbook_YYYY-MM-DD.bin.\n\n        Args:\n            data_path (str): The directory containing the binary data files.\n            verbose (bool, optional): If True, display progress logs. Default is True.\n\n        Processing Steps:\n            - Retrieve the list of binary file paths for the simulation period.\n            - Iterate through each day's data, add the file to the order queue, and run the simulation for that day.\n        \"\"\"\n        start_date = pd.Timestamp(year=self._sim_cpp.params.startYear,\n                                  month=self._sim_cpp.params.startMonth,\n                                  day=self._sim_cpp.params.startDay,\n                                  hour=self._sim_cpp.params.startHour,\n                                  tz=\"UTC\")\n        end_date = pd.Timestamp(year=self._sim_cpp.params.endYear,\n                                month=self._sim_cpp.params.endMonth,\n                                day=self._sim_cpp.params.endDay,\n                                hour=self._sim_cpp.params.endHour,\n                                tz=\"UTC\")\n        lob_paths = self.get_data_bins_for_each_day(data_path, start_date, end_date)\n\n        num_days = len(lob_paths)\n        print(\"The simulation will iterate over\", num_days, \"files.\")\n\n        with tqdm(total=num_days, desc=\"Simulated Days\", unit=\"%\", ncols=120, disable=not verbose) as pbar:\n            for i, path in enumerate(lob_paths):\n                pbar.set_description(f\"Currently simulating {path.split('/')[-1]} ... \")\n                self.add_bin_to_orderqueue(path)\n                self.run_one_day(i == len(lob_paths) - 1)\n                pbar.update(1)\n\n        print(\"Simulation finished.\")\n\n    def run_one_day(self, is_last: bool):\n        \"\"\"\n        Run the simulation for a single day.\n\n        Args:\n            is_last (bool): If True, indicates that this is the last iteration of data.\n\n        Processing Steps:\n            - Execute the simulation for the provided day's data.\n        \"\"\"\n        self._sim_cpp.run(is_last)\n\n    def get_logs(self):\n        \"\"\"\n        Retrieve the logs generated by the simulation.\n\n        Returns:\n            dict: A dictionary containing simulation logs with the following keys:\n                - decision_record: Final simulation schedule.\n                - price_record: CID price data over the simulation duration.\n                - accepted_orders: Limit orders accepted by the RI.\n                - executed_orders: Orders sent to the exchange by the RI.\n                - killed_orders: Orders that were missed at the exchange.\n        \"\"\"\n        # - forecast_orders: Orders virtually traded against the forecast.\n        # - balancing_orders: Orders that would have incurred payments to the TSO.\n        decision_record, price_record, accepted_orders, executed_orders, forecast_orders, killed_orders, balancing_orders = self._sim_cpp.getLogs()\n        decision_record = pd.DataFrame(decision_record)\n        price_record = pd.DataFrame(price_record)\n        accepted_orders = pd.DataFrame(accepted_orders)\n        executed_orders = pd.DataFrame(executed_orders)\n        forecast_orders = pd.DataFrame(forecast_orders)\n        killed_orders = pd.DataFrame(killed_orders)\n        balancing_orders = pd.DataFrame(balancing_orders)\n\n        if not decision_record.empty:\n            decision_record[\"hour\"] = pd.to_datetime(decision_record[\"hour\"], utc=True)\n        if not price_record.empty:\n            price_record[\"hour\"] = pd.to_datetime(price_record[\"hour\"], utc=True)\n        if not accepted_orders.empty:\n            accepted_orders[\"time\"] = pd.to_datetime(accepted_orders[\"time\"], utc=True)\n            accepted_orders[\"start\"] = pd.to_datetime(accepted_orders[\"start\"], utc=True)\n            accepted_orders[\"cancel\"] = pd.to_datetime(accepted_orders[\"cancel\"], utc=True)\n            accepted_orders[\"delivery\"] = pd.to_datetime(accepted_orders[\"delivery\"], utc=True)\n        if not executed_orders.empty:\n            executed_orders[\"time\"] = pd.to_datetime(executed_orders[\"time\"], utc=True)\n            executed_orders[\"last_solve_time\"] = pd.to_datetime(executed_orders[\"last_solve_time\"], utc=True)\n            executed_orders[\"hour\"] = pd.to_datetime(executed_orders[\"hour\"], utc=True)\n        if not forecast_orders.empty:\n            forecast_orders[\"time\"] = pd.to_datetime(forecast_orders[\"time\"], utc=True)\n            forecast_orders[\"last_solve_time\"] = pd.to_datetime(forecast_orders[\"last_solve_time\"], utc=True)\n            forecast_orders[\"hour\"] = pd.to_datetime(forecast_orders[\"hour\"], utc=True)\n        if not killed_orders.empty:\n            killed_orders[\"time\"] = pd.to_datetime(killed_orders[\"time\"], utc=True)\n            killed_orders[\"last_solve_time\"] = pd.to_datetime(killed_orders[\"last_solve_time\"], utc=True)\n            killed_orders[\"hour\"] = pd.to_datetime(killed_orders[\"hour\"], utc=True)\n        if not balancing_orders.empty:\n            balancing_orders[\"time\"] = pd.to_datetime(balancing_orders[\"time\"], utc=True)\n            balancing_orders[\"hour\"] = pd.to_datetime(balancing_orders[\"hour\"], utc=True)\n\n        logs = {\n            \"decision_record\": pd.DataFrame(decision_record, index=None),\n            \"price_record\": pd.DataFrame(price_record, index=None),\n            \"accepted_orders\": pd.DataFrame(accepted_orders, index=None),\n            \"executed_orders\": pd.DataFrame(executed_orders, index=None),\n            # \"forecast_orders\": pd.DataFrame(forecast_orders, index=None), # removed for later versions of the code\n            \"killed_orders\": pd.DataFrame(killed_orders, index=None),\n            # \"balancing_orders\": pd.DataFrame(balancing_orders, index=None), # removed for later versions of the code\n        }\n        return logs\n\n    def print_parameters(self):\n        \"\"\"\n        Print the simulation parameters, including start/end times, storage settings, and various limits and costs.\n        \"\"\"\n        startMonth = self._sim_cpp.params.startMonth\n        startDay = self._sim_cpp.params.startDay\n        startYear = self._sim_cpp.params.startYear\n        startHour = self._sim_cpp.params.startHour\n        endMonth = self._sim_cpp.params.endMonth\n        endDay = self._sim_cpp.params.endDay\n        endYear = self._sim_cpp.params.endYear\n        endHour = self._sim_cpp.params.endHour\n\n        startDate = pd.Timestamp(year=startYear, month=startMonth, day=startDay, hour=startHour, tz=\"UTC\")\n        endDate = pd.Timestamp(year=endYear, month=endMonth, day=endDay, hour=endHour, tz=\"UTC\")\n        print(\"Start Time (UTC):\", startDate)\n        print(\"End Time (UTC):\", endDate)\n\n        print(\"Storage Maximum:\", self._sim_cpp.params.storageMax, \"MWh\")\n        print(\"Linear Degredation Cost:\", self._sim_cpp.params.linDegCost, \"\u20ac/MWh\")\n        print(\"Injection Loss \u03b7+:\", self._sim_cpp.params.lossIn)\n        print(\"Withdrawal Loss \u03b7-:\", self._sim_cpp.params.lossOut)\n        print(\"Trading Fee:\", self._sim_cpp.params.tradingFee, \"\u20ac/MWh\")\n        print(\"Number of DP Storage States:\", self._sim_cpp.params.numStorStates)\n        print(\"Technical Delay:\", self._sim_cpp.params.pingDelay, \"ms\")\n        print(\"Fixed Solve Time:\", self._sim_cpp.params.fixedSolveTime, \"ms\")\n        print(\"Solve Frequency:\", self._sim_cpp.params.dpFreq, \"min\")\n        print(\"Injection Maximum:\", self._sim_cpp.params.injectMax, \"MW\")\n        print(\"Withdrawal Maximum:\", self._sim_cpp.params.withdrawMax, \"MW\")\n        # print(\"Forecast Horizon Start:\", self._sim_cpp.params.foreHorizonStart, \"min\")\n        # print(\"Forecast Horizon End:\", self._sim_cpp.params.foreHorizonEnd, \"min\")\n\n    def return_vol_price_pairs(self, is_last: bool, frequency: int, volumes: np.ndarray):\n        \"\"\"\n        Retrieve volume-price pairs from the simulation.\n\n        Args:\n            is_last (bool): If True, indicates this is the last iteration of data.\n            frequency (int): The frequency (in seconds) at which price data is retrieved.\n            volumes (np.ndarray): A 1D numpy array of volumes for which prices are returned.\n\n        Returns:\n            pd.DataFrame: A DataFrame with columns:\n                - current_time: Time of the export (UTC).\n                - delivery_hour: Delivery period time (UTC).\n                - volume: The volume for which the price is exported (MWh).\n                - price_full: The full price (cashflow) for the volume (\u20ac).\n                - worst_accepted_price: Market price of the worst matched order (\u20ac/MWh).\n        \"\"\"\n        if len(volumes.shape) != 1:\n            raise ValueError(\"volumes must be a 1D numpy array\")\n        if frequency &lt;= 0:\n            raise ValueError(\"frequency must be &gt; 0\")\n\n        vol_price_list = self._sim_cpp.return_vol_price_pairs(is_last, frequency, volumes)\n        vol_price_list = pd.DataFrame(vol_price_list)\n\n        if not vol_price_list.empty:\n            vol_price_list[\"current_time\"] = pd.to_datetime(vol_price_list[\"current_time\"], utc=True)\n            vol_price_list[\"delivery_hour\"] = pd.to_datetime(vol_price_list[\"delivery_hour\"], utc=True)\n\n        return vol_price_list\n</code></pre>"},{"location":"simulation/#bitepy.Simulation.__init__","title":"<code>__init__(start_date, end_date, storage_max=10.0, lin_deg_cost=4.0, loss_in=0.95, loss_out=0.95, trading_fee=0.09, num_stor_states=11, tec_delay=0, fixed_solve_time=0, solve_frequency=0.0, withdraw_max=10.0, inject_max=10.0)</code>","text":"<p>Initialize a Simulation instance.</p> <p>Parameters:</p> Name Type Description Default <code>start_date</code> <code>Timestamp</code> <p>The start datetime of the simulation. Must be timezone aware.</p> required <code>end_date</code> <code>Timestamp</code> <p>The end datetime of the simulation. Must be timezone aware.</p> required <code>storage_max</code> <code>float</code> <p>The maximum storage capacity of the storage unit (MWh). Default is 10.0.</p> <code>10.0</code> <code>lin_deg_cost</code> <code>float</code> <p>The linear degradation cost of the storage unit (\u20ac/MWh). Default is 4.0.</p> <code>4.0</code> <code>loss_in</code> <code>float</code> <p>The injection efficiency of the storage unit (0-1]. Default is 0.95.</p> <code>0.95</code> <code>loss_out</code> <code>float</code> <p>The withdrawal efficiency of the storage unit (0-1]. Default is 0.95.</p> <code>0.95</code> <code>trading_fee</code> <code>float</code> <p>The trading fee for the exchange (\u20ac/MWh). Default is 0.09.</p> <code>0.09</code> <code>num_stor_states</code> <code>int</code> <p>The number of storage states for dynamic programming. Default is 11.</p> <code>11</code> <code>tec_delay</code> <code>int</code> <p>The technical delay of the storage unit (ms, &gt;= 0). Default is 0.</p> <code>0</code> <code>fixed_solve_time</code> <code>int</code> <p>The fixed solve time for dynamic programming (ms, &gt;= 0 or -1 for realistic solve times). Default is 0.</p> <code>0</code> <code>solve_frequency</code> <code>float</code> <p>The frequency at which the dynamic programming solver is run (min). Default is 0.0.</p> <code>0.0</code> <code>withdraw_max</code> <code>float</code> <p>The maximum withdrawal power of the storage unit (MW). Default is 10.0.</p> <code>10.0</code> <code>inject_max</code> <code>float</code> <p>The maximum injection power of the storage unit (MW). Default is 10.0.</p> <code>10.0</code> Source code in <code>bitepy/simulation.py</code> <pre><code>def __init__(self, start_date: pd.Timestamp, end_date: pd.Timestamp,\n             storage_max=10.,\n             lin_deg_cost=4.,\n             loss_in=0.95,\n             loss_out=0.95,\n             trading_fee=0.09,\n             num_stor_states=11,\n             tec_delay=0,\n             fixed_solve_time=0,\n             solve_frequency=0.,\n             withdraw_max=10.,\n             inject_max=10.):\n            #  forecast_horizon_start=10*60,\n            #  forecast_horizon_end=75):\n    \"\"\"\n    Initialize a Simulation instance.\n\n    Args:\n        start_date (pd.Timestamp): The start datetime of the simulation. Must be timezone aware.\n        end_date (pd.Timestamp): The end datetime of the simulation. Must be timezone aware.\n        storage_max (float, optional): The maximum storage capacity of the storage unit (MWh). Default is 10.0.\n        lin_deg_cost (float, optional): The linear degradation cost of the storage unit (\u20ac/MWh). Default is 4.0.\n        loss_in (float, optional): The injection efficiency of the storage unit (0-1]. Default is 0.95.\n        loss_out (float, optional): The withdrawal efficiency of the storage unit (0-1]. Default is 0.95.\n        trading_fee (float, optional): The trading fee for the exchange (\u20ac/MWh). Default is 0.09.\n        num_stor_states (int, optional): The number of storage states for dynamic programming. Default is 11.\n        tec_delay (int, optional): The technical delay of the storage unit (ms, &gt;= 0). Default is 0.\n        fixed_solve_time (int, optional): The fixed solve time for dynamic programming (ms, &gt;= 0 or -1 for realistic solve times). Default is 0.\n        solve_frequency (float, optional): The frequency at which the dynamic programming solver is run (min). Default is 0.0.\n        withdraw_max (float, optional): The maximum withdrawal power of the storage unit (MW). Default is 10.0.\n        inject_max (float, optional): The maximum injection power of the storage unit (MW). Default is 10.0.\n    \"\"\"\n    # forecast_horizon_start (int, optional): The start of the forecast horizon (min). Default is 600.\n    # forecast_horizon_end (int, optional): The end of the forecast horizon (min). Default is 75.\n\n    # write all the assertions\n    if start_date &gt;= end_date:\n        raise ValueError(\"start_date must be before end_date\")\n    if storage_max &lt; 0:\n        raise ValueError(\"storage_max must be &gt;= 0\")\n    if lin_deg_cost &lt; 0:\n        raise ValueError(\"lin_deg_cost must be &gt;= 0\")\n    if loss_in &lt; 0 or loss_in &gt; 1:\n        raise ValueError(\"loss_in must be in [0, 1]\")\n    if loss_out &lt; 0 or loss_out &gt; 1:\n        raise ValueError(\"loss_out must be in [0,1]\")\n    if trading_fee &lt; 0:\n        raise ValueError(\"trading_fee must be &gt;= 0\")\n    if num_stor_states &lt;= 0:\n        raise ValueError(\"num_stor_states must be &gt; 0\")\n    if tec_delay &lt; 0:\n        raise ValueError(\"tec_delay must be &gt;= 0\")\n    if fixed_solve_time &lt; 0:\n        if fixed_solve_time != -1:\n            raise ValueError(\"fixed_solve_time must be &gt;= 0 (or -1 for realistic solve times)\")\n    if solve_frequency &lt; 0:\n        raise ValueError(\"solve_frequency must be &gt;= 0\")\n    if withdraw_max &lt;= 0:\n        raise ValueError(\"withdraw_max must be &gt; 0\")\n    if inject_max &lt;= 0:\n        raise ValueError(\"inject_max must be &gt; 0\")\n    # if forecast_horizon_start &lt; 0:\n    #     raise ValueError(\"forecast_horizon_start must be &gt;= 0\")\n    # if forecast_horizon_end &lt; 0:\n    #     raise ValueError(\"forecast_horizon_end must be &gt;= 0\")\n    # if forecast_horizon_start &lt;= forecast_horizon_end:\n    #     raise ValueError(\"forecast_horizon_start must larger than forecast_horizon_end\")\n\n    self._sim_cpp = Simulation_cpp()\n\n    self._sim_cpp.params.storageMax = storage_max\n    self._sim_cpp.params.linDegCost = lin_deg_cost\n    self._sim_cpp.params.lossIn = loss_in\n    self._sim_cpp.params.lossOut = loss_out\n    self._sim_cpp.params.tradingFee = trading_fee\n    self._sim_cpp.params.numStorStates = num_stor_states\n    self._sim_cpp.params.pingDelay = tec_delay\n    self._sim_cpp.params.fixedSolveTime = fixed_solve_time\n    self._sim_cpp.params.dpFreq = solve_frequency\n    self._sim_cpp.params.withdrawMax = withdraw_max\n    self._sim_cpp.params.injectMax = inject_max\n    # self._sim_cpp.params.foreHorizonStart = forecast_horizon_start\n    # self._sim_cpp.params.foreHorizonEnd = forecast_horizon_end\n\n    # Set start and end date\n    if start_date &gt;= end_date:\n        raise ValueError(\"start_date must be before end_date\")\n    if start_date.tzinfo is None:\n        raise ValueError(\"start_date must be timezone aware\")\n    start_date = start_date.astimezone(pytz.utc)\n    self._sim_cpp.params.startMonth = start_date.month\n    self._sim_cpp.params.startDay = start_date.day\n    self._sim_cpp.params.startYear = start_date.year\n    self._sim_cpp.params.startHour = start_date.hour\n    if end_date.tzinfo is None:\n        raise ValueError(\"end_date must be timezone aware\")\n    end_date = end_date.astimezone(pytz.utc)\n    self._sim_cpp.params.endMonth = end_date.month\n    self._sim_cpp.params.endDay = end_date.day\n    self._sim_cpp.params.endYear = end_date.year\n    self._sim_cpp.params.endHour = end_date.hour\n</code></pre>"},{"location":"simulation/#bitepy.Simulation.add_bin_to_orderqueue","title":"<code>add_bin_to_orderqueue(bin_data)</code>","text":"<p>Add an order binary file to the simulation's order queue.</p> <p>Parameters:</p> Name Type Description Default <code>bin_data</code> <code>str</code> <p>The path to the order binary file.</p> required Source code in <code>bitepy/simulation.py</code> <pre><code>def add_bin_to_orderqueue(self, bin_data: str):\n    \"\"\"\n    Add an order binary file to the simulation's order queue.\n\n    Args:\n        bin_data (str): The path to the order binary file.\n    \"\"\"\n    self._sim_cpp.addOrderQueueFromBin(bin_data)\n</code></pre>"},{"location":"simulation/#bitepy.Simulation.add_df_to_orderqueue","title":"<code>add_df_to_orderqueue(df)</code>","text":"<p>Add a DataFrame of orders to the simulation's order queue.</p> <p>The DataFrame must have the same columns as the saved CSV files, with timestamps in UTC (seconds and milliseconds).</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A DataFrame containing the orders to be added.</p> required Processing Steps <ul> <li>Validate that the timestamp columns ('start', 'transaction', 'validity') are timezone aware.</li> <li>Ensure that all timestamps are in the same timezone.</li> <li>Convert all timestamps to UTC and format them in ISO 8601.</li> </ul> Source code in <code>bitepy/simulation.py</code> <pre><code>def add_df_to_orderqueue(self, df: pd.DataFrame):\n    \"\"\"\n    Add a DataFrame of orders to the simulation's order queue.\n\n    The DataFrame must have the same columns as the saved CSV files, with timestamps in UTC\n    (seconds and milliseconds).\n\n    Args:\n        df (pd.DataFrame): A DataFrame containing the orders to be added.\n\n    Processing Steps:\n        - Validate that the timestamp columns ('start', 'transaction', 'validity') are timezone aware.\n        - Ensure that all timestamps are in the same timezone.\n        - Convert all timestamps to UTC and format them in ISO 8601.\n    \"\"\"\n    if (df[\"start\"].dt.tz is None and df[\"transaction\"].dt.tz is None and df[\"validity\"].dt.tz is None):\n        raise ValueError(\"All timestamps of input df must be timezone aware\")\n    if not (df[\"start\"].dt.tz == df[\"transaction\"].dt.tz and df[\"start\"].dt.tz == df[\"validity\"].dt.tz):\n        raise ValueError(\"All timestamps of input df must be in the same timezone\")\n\n    df[\"start\"] = df[\"start\"].dt.tz_convert(\"UTC\")\n    df[\"transaction\"] = df[\"transaction\"].dt.tz_convert(\"UTC\")\n    df[\"validity\"] = df[\"validity\"].dt.tz_convert(\"UTC\")\n    df[\"start\"] = df[\"start\"].dt.tz_localize(None).dt.strftime('%Y-%m-%dT%H:%M:%SZ')\n    df[\"transaction\"] = df[\"transaction\"].dt.tz_localize(None).dt.strftime('%Y-%m-%dT%H:%M:%S.%f').str[:-3] + 'Z'\n    df[\"validity\"] = df[\"validity\"].dt.tz_localize(None).dt.strftime('%Y-%m-%dT%H:%M:%S.%f').str[:-3] + 'Z'\n\n    ids = df['id'].to_numpy(dtype=np.int64).tolist()\n    initials = df['initial'].to_numpy(dtype=np.int64).tolist()\n    sides = df['side'].to_numpy(dtype='str').tolist()\n    starts = df['start'].to_numpy(dtype='str').tolist()\n    transactions = df['transaction'].to_numpy(dtype='str').tolist()\n    validities = df['validity'].to_numpy(dtype='str').tolist()\n    prices = df['price'].to_numpy(dtype=np.float64).tolist()\n    quantities = df['quantity'].to_numpy(dtype=np.float64).tolist()\n\n    self._sim_cpp.addOrderQueueFromPandas(ids, initials, sides, starts, transactions, validities, prices, quantities)\n</code></pre>"},{"location":"simulation/#bitepy.Simulation.get_data_bins_for_each_day","title":"<code>get_data_bins_for_each_day(base_path, start_date, end_date)</code>","text":"<p>Generate a list of file paths for binary order book data for each day within a date range.</p> <p>Parameters:</p> Name Type Description Default <code>base_path</code> <code>str</code> <p>The base directory path where the binary files are stored.</p> required <code>start_date</code> <code>Timestamp</code> <p>The start date of the range.</p> required <code>end_date</code> <code>Timestamp</code> <p>The end date of the range.</p> required <p>Returns:</p> Name Type Description <code>list</code> <p>A list of file paths for each day's binary order book file.</p> Source code in <code>bitepy/simulation.py</code> <pre><code>def get_data_bins_for_each_day(self, base_path: str, start_date: pd.Timestamp, end_date: pd.Timestamp):\n    \"\"\"\n    Generate a list of file paths for binary order book data for each day within a date range.\n\n    Args:\n        base_path (str): The base directory path where the binary files are stored.\n        start_date (pd.Timestamp): The start date of the range.\n        end_date (pd.Timestamp): The end date of the range.\n\n    Returns:\n        list: A list of file paths for each day's binary order book file.\n    \"\"\"\n    start_date_utc = start_date.tz_convert('UTC')\n    end_date_utc = end_date.tz_convert('UTC')\n\n    if base_path[-1] != '/':\n        base_path += '/'\n    base_path += \"orderbook_\"\n\n    paths = []\n    current_date = start_date_utc\n    while current_date &lt; end_date_utc + timedelta(days=1):\n        path = f\"{base_path}{current_date.strftime('%Y-%m-%d')}.bin\"\n        paths.append(path)\n        current_date += timedelta(days=1)\n    return paths\n</code></pre>"},{"location":"simulation/#bitepy.Simulation.get_logs","title":"<code>get_logs()</code>","text":"<p>Retrieve the logs generated by the simulation.</p> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary containing simulation logs with the following keys: - decision_record: Final simulation schedule. - price_record: CID price data over the simulation duration. - accepted_orders: Limit orders accepted by the RI. - executed_orders: Orders sent to the exchange by the RI. - killed_orders: Orders that were missed at the exchange.</p> Source code in <code>bitepy/simulation.py</code> <pre><code>def get_logs(self):\n    \"\"\"\n    Retrieve the logs generated by the simulation.\n\n    Returns:\n        dict: A dictionary containing simulation logs with the following keys:\n            - decision_record: Final simulation schedule.\n            - price_record: CID price data over the simulation duration.\n            - accepted_orders: Limit orders accepted by the RI.\n            - executed_orders: Orders sent to the exchange by the RI.\n            - killed_orders: Orders that were missed at the exchange.\n    \"\"\"\n    # - forecast_orders: Orders virtually traded against the forecast.\n    # - balancing_orders: Orders that would have incurred payments to the TSO.\n    decision_record, price_record, accepted_orders, executed_orders, forecast_orders, killed_orders, balancing_orders = self._sim_cpp.getLogs()\n    decision_record = pd.DataFrame(decision_record)\n    price_record = pd.DataFrame(price_record)\n    accepted_orders = pd.DataFrame(accepted_orders)\n    executed_orders = pd.DataFrame(executed_orders)\n    forecast_orders = pd.DataFrame(forecast_orders)\n    killed_orders = pd.DataFrame(killed_orders)\n    balancing_orders = pd.DataFrame(balancing_orders)\n\n    if not decision_record.empty:\n        decision_record[\"hour\"] = pd.to_datetime(decision_record[\"hour\"], utc=True)\n    if not price_record.empty:\n        price_record[\"hour\"] = pd.to_datetime(price_record[\"hour\"], utc=True)\n    if not accepted_orders.empty:\n        accepted_orders[\"time\"] = pd.to_datetime(accepted_orders[\"time\"], utc=True)\n        accepted_orders[\"start\"] = pd.to_datetime(accepted_orders[\"start\"], utc=True)\n        accepted_orders[\"cancel\"] = pd.to_datetime(accepted_orders[\"cancel\"], utc=True)\n        accepted_orders[\"delivery\"] = pd.to_datetime(accepted_orders[\"delivery\"], utc=True)\n    if not executed_orders.empty:\n        executed_orders[\"time\"] = pd.to_datetime(executed_orders[\"time\"], utc=True)\n        executed_orders[\"last_solve_time\"] = pd.to_datetime(executed_orders[\"last_solve_time\"], utc=True)\n        executed_orders[\"hour\"] = pd.to_datetime(executed_orders[\"hour\"], utc=True)\n    if not forecast_orders.empty:\n        forecast_orders[\"time\"] = pd.to_datetime(forecast_orders[\"time\"], utc=True)\n        forecast_orders[\"last_solve_time\"] = pd.to_datetime(forecast_orders[\"last_solve_time\"], utc=True)\n        forecast_orders[\"hour\"] = pd.to_datetime(forecast_orders[\"hour\"], utc=True)\n    if not killed_orders.empty:\n        killed_orders[\"time\"] = pd.to_datetime(killed_orders[\"time\"], utc=True)\n        killed_orders[\"last_solve_time\"] = pd.to_datetime(killed_orders[\"last_solve_time\"], utc=True)\n        killed_orders[\"hour\"] = pd.to_datetime(killed_orders[\"hour\"], utc=True)\n    if not balancing_orders.empty:\n        balancing_orders[\"time\"] = pd.to_datetime(balancing_orders[\"time\"], utc=True)\n        balancing_orders[\"hour\"] = pd.to_datetime(balancing_orders[\"hour\"], utc=True)\n\n    logs = {\n        \"decision_record\": pd.DataFrame(decision_record, index=None),\n        \"price_record\": pd.DataFrame(price_record, index=None),\n        \"accepted_orders\": pd.DataFrame(accepted_orders, index=None),\n        \"executed_orders\": pd.DataFrame(executed_orders, index=None),\n        # \"forecast_orders\": pd.DataFrame(forecast_orders, index=None), # removed for later versions of the code\n        \"killed_orders\": pd.DataFrame(killed_orders, index=None),\n        # \"balancing_orders\": pd.DataFrame(balancing_orders, index=None), # removed for later versions of the code\n    }\n    return logs\n</code></pre>"},{"location":"simulation/#bitepy.Simulation.print_parameters","title":"<code>print_parameters()</code>","text":"<p>Print the simulation parameters, including start/end times, storage settings, and various limits and costs.</p> Source code in <code>bitepy/simulation.py</code> <pre><code>def print_parameters(self):\n    \"\"\"\n    Print the simulation parameters, including start/end times, storage settings, and various limits and costs.\n    \"\"\"\n    startMonth = self._sim_cpp.params.startMonth\n    startDay = self._sim_cpp.params.startDay\n    startYear = self._sim_cpp.params.startYear\n    startHour = self._sim_cpp.params.startHour\n    endMonth = self._sim_cpp.params.endMonth\n    endDay = self._sim_cpp.params.endDay\n    endYear = self._sim_cpp.params.endYear\n    endHour = self._sim_cpp.params.endHour\n\n    startDate = pd.Timestamp(year=startYear, month=startMonth, day=startDay, hour=startHour, tz=\"UTC\")\n    endDate = pd.Timestamp(year=endYear, month=endMonth, day=endDay, hour=endHour, tz=\"UTC\")\n    print(\"Start Time (UTC):\", startDate)\n    print(\"End Time (UTC):\", endDate)\n\n    print(\"Storage Maximum:\", self._sim_cpp.params.storageMax, \"MWh\")\n    print(\"Linear Degredation Cost:\", self._sim_cpp.params.linDegCost, \"\u20ac/MWh\")\n    print(\"Injection Loss \u03b7+:\", self._sim_cpp.params.lossIn)\n    print(\"Withdrawal Loss \u03b7-:\", self._sim_cpp.params.lossOut)\n    print(\"Trading Fee:\", self._sim_cpp.params.tradingFee, \"\u20ac/MWh\")\n    print(\"Number of DP Storage States:\", self._sim_cpp.params.numStorStates)\n    print(\"Technical Delay:\", self._sim_cpp.params.pingDelay, \"ms\")\n    print(\"Fixed Solve Time:\", self._sim_cpp.params.fixedSolveTime, \"ms\")\n    print(\"Solve Frequency:\", self._sim_cpp.params.dpFreq, \"min\")\n    print(\"Injection Maximum:\", self._sim_cpp.params.injectMax, \"MW\")\n    print(\"Withdrawal Maximum:\", self._sim_cpp.params.withdrawMax, \"MW\")\n</code></pre>"},{"location":"simulation/#bitepy.Simulation.return_vol_price_pairs","title":"<code>return_vol_price_pairs(is_last, frequency, volumes)</code>","text":"<p>Retrieve volume-price pairs from the simulation.</p> <p>Parameters:</p> Name Type Description Default <code>is_last</code> <code>bool</code> <p>If True, indicates this is the last iteration of data.</p> required <code>frequency</code> <code>int</code> <p>The frequency (in seconds) at which price data is retrieved.</p> required <code>volumes</code> <code>ndarray</code> <p>A 1D numpy array of volumes for which prices are returned.</p> required <p>Returns:</p> Type Description <p>pd.DataFrame: A DataFrame with columns: - current_time: Time of the export (UTC). - delivery_hour: Delivery period time (UTC). - volume: The volume for which the price is exported (MWh). - price_full: The full price (cashflow) for the volume (\u20ac). - worst_accepted_price: Market price of the worst matched order (\u20ac/MWh).</p> Source code in <code>bitepy/simulation.py</code> <pre><code>def return_vol_price_pairs(self, is_last: bool, frequency: int, volumes: np.ndarray):\n    \"\"\"\n    Retrieve volume-price pairs from the simulation.\n\n    Args:\n        is_last (bool): If True, indicates this is the last iteration of data.\n        frequency (int): The frequency (in seconds) at which price data is retrieved.\n        volumes (np.ndarray): A 1D numpy array of volumes for which prices are returned.\n\n    Returns:\n        pd.DataFrame: A DataFrame with columns:\n            - current_time: Time of the export (UTC).\n            - delivery_hour: Delivery period time (UTC).\n            - volume: The volume for which the price is exported (MWh).\n            - price_full: The full price (cashflow) for the volume (\u20ac).\n            - worst_accepted_price: Market price of the worst matched order (\u20ac/MWh).\n    \"\"\"\n    if len(volumes.shape) != 1:\n        raise ValueError(\"volumes must be a 1D numpy array\")\n    if frequency &lt;= 0:\n        raise ValueError(\"frequency must be &gt; 0\")\n\n    vol_price_list = self._sim_cpp.return_vol_price_pairs(is_last, frequency, volumes)\n    vol_price_list = pd.DataFrame(vol_price_list)\n\n    if not vol_price_list.empty:\n        vol_price_list[\"current_time\"] = pd.to_datetime(vol_price_list[\"current_time\"], utc=True)\n        vol_price_list[\"delivery_hour\"] = pd.to_datetime(vol_price_list[\"delivery_hour\"], utc=True)\n\n    return vol_price_list\n</code></pre>"},{"location":"simulation/#bitepy.Simulation.run","title":"<code>run(data_path, verbose=True)</code>","text":"<p>Execute the simulation using binary data files.</p> <p>The files must be named as: orderbook_YYYY-MM-DD.bin.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>str</code> <p>The directory containing the binary data files.</p> required <code>verbose</code> <code>bool</code> <p>If True, display progress logs. Default is True.</p> <code>True</code> Processing Steps <ul> <li>Retrieve the list of binary file paths for the simulation period.</li> <li>Iterate through each day's data, add the file to the order queue, and run the simulation for that day.</li> </ul> Source code in <code>bitepy/simulation.py</code> <pre><code>def run(self, data_path: str, verbose: bool = True):\n    \"\"\"\n    Execute the simulation using binary data files.\n\n    The files must be named as: orderbook_YYYY-MM-DD.bin.\n\n    Args:\n        data_path (str): The directory containing the binary data files.\n        verbose (bool, optional): If True, display progress logs. Default is True.\n\n    Processing Steps:\n        - Retrieve the list of binary file paths for the simulation period.\n        - Iterate through each day's data, add the file to the order queue, and run the simulation for that day.\n    \"\"\"\n    start_date = pd.Timestamp(year=self._sim_cpp.params.startYear,\n                              month=self._sim_cpp.params.startMonth,\n                              day=self._sim_cpp.params.startDay,\n                              hour=self._sim_cpp.params.startHour,\n                              tz=\"UTC\")\n    end_date = pd.Timestamp(year=self._sim_cpp.params.endYear,\n                            month=self._sim_cpp.params.endMonth,\n                            day=self._sim_cpp.params.endDay,\n                            hour=self._sim_cpp.params.endHour,\n                            tz=\"UTC\")\n    lob_paths = self.get_data_bins_for_each_day(data_path, start_date, end_date)\n\n    num_days = len(lob_paths)\n    print(\"The simulation will iterate over\", num_days, \"files.\")\n\n    with tqdm(total=num_days, desc=\"Simulated Days\", unit=\"%\", ncols=120, disable=not verbose) as pbar:\n        for i, path in enumerate(lob_paths):\n            pbar.set_description(f\"Currently simulating {path.split('/')[-1]} ... \")\n            self.add_bin_to_orderqueue(path)\n            self.run_one_day(i == len(lob_paths) - 1)\n            pbar.update(1)\n\n    print(\"Simulation finished.\")\n</code></pre>"},{"location":"simulation/#bitepy.Simulation.run_one_day","title":"<code>run_one_day(is_last)</code>","text":"<p>Run the simulation for a single day.</p> <p>Parameters:</p> Name Type Description Default <code>is_last</code> <code>bool</code> <p>If True, indicates that this is the last iteration of data.</p> required Processing Steps <ul> <li>Execute the simulation for the provided day's data.</li> </ul> Source code in <code>bitepy/simulation.py</code> <pre><code>def run_one_day(self, is_last: bool):\n    \"\"\"\n    Run the simulation for a single day.\n\n    Args:\n        is_last (bool): If True, indicates that this is the last iteration of data.\n\n    Processing Steps:\n        - Execute the simulation for the provided day's data.\n    \"\"\"\n    self._sim_cpp.run(is_last)\n</code></pre>"}]}